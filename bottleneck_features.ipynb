{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img  \n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dropout, Flatten, Dense  \n",
    "from keras import applications  \n",
    "from keras.utils.np_utils import to_categorical  \n",
    "import matplotlib.pyplot as plt  \n",
    "import math  \n",
    "import cv2  \n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    filename= \"C:/Users/Komal/Desktop/classes/mlp/Kobe/data.csv\"\n",
    "    raw = pd.read_csv(filename,  index_col=None)\n",
    "    # drops = ['shot_id', 'team_id', 'team_name', 'shot_zone_area', 'shot_zone_range', 'shot_zone_basic', \\\n",
    "             # 'matchup', 'game_event_id', 'game_id', 'game_date']\n",
    "    # drops = ['team_id', 'team_name', 'matchup', 'game_id', 'game_date']\n",
    "    drops = ['team_id', 'team_name', 'game_event_id', 'game_id', 'game_date', 'shot_id']\n",
    "    for drop in drops:\n",
    "        raw = raw.drop(drop, 1)\n",
    "    # categorical_vars = ['action_type', 'combined_shot_type', 'shot_type', 'opponent', 'season']\n",
    "    # categorical_vars = ['action_type', 'combined_shot_type', 'shot_type', 'opponent', 'season', 'shot_id', 'shot_zone_area', 'shot_zone_range', 'shot_zone_basic', \\\n",
    "    #          'matchup', 'game_event_id', 'game_date']\n",
    "    categorical_vars = ['action_type', 'combined_shot_type', 'shot_type', 'shot_zone_area', 'shot_zone_range', 'shot_zone_basic', 'opponent', 'season']\n",
    "    for var in categorical_vars:\n",
    "        raw = pd.concat([raw, pd.get_dummies(raw[var], prefix=var)], 1)\n",
    "        raw = raw.drop(var, 1)\n",
    "    for i, row in enumerate(raw.itertuples(), 1):\n",
    "        if \"@\" in row.matchup:\n",
    "            raw.set_value(row.Index, 'matchup', 0)\n",
    "        else:\n",
    "            raw.set_value(row.Index, 'matchup', 1)\n",
    "    df = raw[pd.notnull(raw['shot_made_flag'])]\n",
    "    indexOfNull = raw[raw['shot_made_flag'].isnull()].index.tolist()\n",
    "    return raw, df, indexOfNull\n",
    "\n",
    "def split_data(raw, df, indexOfNull):\n",
    "    per_test =  round((15 * len(df))/100)\n",
    "    n =[0 for i in range(per_test)]\n",
    "    m = len(indexOfNull)-1\n",
    "    for z in range(per_test):\n",
    "        c = indexOfNull[m]+1\n",
    "        x = raw.iloc[c]\n",
    "        #print(pd.notnull(x['shot_made_flag']))\n",
    "        flg = 0\n",
    "        if(pd.isnull(x['shot_made_flag'])):\n",
    "            #print(c)\n",
    "            flg =1\n",
    "            while flg==1:\n",
    "                c = c+1\n",
    "                x = raw.iloc[c]\n",
    "                if pd.notnull(x['shot_made_flag']):\n",
    "                    flg =0\n",
    "        n[z] = c\n",
    "        m=m-1;\n",
    "    test_comp = raw.iloc[n]\n",
    "    test = test_comp.drop('shot_made_flag', 1)\n",
    "    test_y = test_comp['shot_made_flag']\n",
    "    df = raw.drop(raw.index[n])\n",
    "    df= df[pd.notnull(df['shot_made_flag'])]\n",
    "    # separate df into explanatory and response variables\n",
    "    train = df.drop('shot_made_flag', 1)\n",
    "    train_y = df['shot_made_flag']\n",
    "    return train, train_y, test, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Komal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "C:\\Users\\Komal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "raw, df, indexOfNull = process_data()\n",
    "x_train, y_train, x_test, y_test = split_data(raw, df, indexOfNull)\n",
    "#print(x_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "def build_model():\n",
    "    num_sensors=1\n",
    "    TIME_PERIODS = 146\n",
    "    num_classes = 1\n",
    "\n",
    "    model_m = Sequential()\n",
    "\n",
    "    model_m.add(Reshape((TIME_PERIODS, num_sensors), input_shape=(x_train.shape[1],)))\n",
    "\n",
    "    model_m.add(Conv1D(200, 73, activation='tanh', input_shape=(TIME_PERIODS, num_sensors), kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model_m.add(Conv1D(90, 55, activation='tanh'))\n",
    "    model_m.add(MaxPooling1D(2,2))\n",
    "    model_m.add(Dropout(0.25))\n",
    "\n",
    "    model_m.add(Conv1D(300, 10, activation='tanh'))\n",
    "    model_m.add(Conv1D(110, 1, activation='tanh'))\n",
    "    model_m.add(MaxPooling1D(1,1))\n",
    "    model_m.add(Dropout(0.25))\n",
    "\n",
    "    model_m.add(Conv1D(110, 1, activation='tanh'))\n",
    "    model_m.add(Conv1D(80, 1, activation='tanh'))\n",
    "    model_m.add(MaxPooling1D(1,1))\n",
    "    model_m.add(Dropout(0.25))\n",
    "\n",
    "    model_m.add(Conv1D(80, 1, activation='tanh'))\n",
    "    model_m.add(Conv1D(40, 1, activation='tanh'))\n",
    "    model_m.add(MaxPooling1D(1,1))\n",
    "    model_m.add(Dropout(0.25))\n",
    "\n",
    "    model_m.add(GlobalAveragePooling1D())\n",
    "\n",
    "    model_m.add(Dropout(0.5))\n",
    "\n",
    "    model_m.add(Dense(num_classes, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model_m.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    print(model_m.summary())\n",
    "    return model_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_save_model_weights(model_m):\n",
    "    \n",
    "    callbacks_list = [\n",
    "    ModelCheckpoint(\n",
    "        filepath='best_model.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "        monitor='val_loss', save_best_only=True),EarlyStopping(monitor='acc', patience=1)\n",
    "    ]\n",
    "    BATCH_SIZE = 400\n",
    "    EPOCHS = 50\n",
    "    history = model_m.fit(x_train, y_train, batch_size=BATCH_SIZE,epochs=EPOCHS,validation_split=0.2, verbose=1)\n",
    "    model_m.save_weights('my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_1 (Reshape)          (None, 146, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 74, 200)           14800     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 20, 90)            990090    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 10, 90)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 90)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 300)            270300    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 1, 110)            33110     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1, 110)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 110)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 1, 110)            12210     \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 1, 80)             8880      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1, 80)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 80)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 1, 80)             6480      \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 1, 40)             3240      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1, 40)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1, 40)             0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 1,339,151\n",
      "Trainable params: 1,339,151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 17954 samples, validate on 4489 samples\n",
      "Epoch 1/50\n",
      "17954/17954 [==============================] - 142s 8ms/step - loss: 0.7172 - acc: 0.5395 - val_loss: 0.6963 - val_acc: 0.5502\n",
      "Epoch 2/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.6964 - acc: 0.5606 - val_loss: 0.6850 - val_acc: 0.6146\n",
      "Epoch 3/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.6836 - acc: 0.6056 - val_loss: 0.6443 - val_acc: 0.6772\n",
      "Epoch 4/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.6510 - acc: 0.6693 - val_loss: 0.6321 - val_acc: 0.6792\n",
      "Epoch 5/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.6403 - acc: 0.6757 - val_loss: 0.6249 - val_acc: 0.6772\n",
      "Epoch 6/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.6358 - acc: 0.6746 - val_loss: 0.6209 - val_acc: 0.6792\n",
      "Epoch 7/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.6361 - acc: 0.6736 - val_loss: 0.6235 - val_acc: 0.6814\n",
      "Epoch 8/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.6297 - acc: 0.6786 - val_loss: 0.6265 - val_acc: 0.6823\n",
      "Epoch 9/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.6268 - acc: 0.6798 - val_loss: 0.6303 - val_acc: 0.6794\n",
      "Epoch 10/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.6272 - acc: 0.6798 - val_loss: 0.6181 - val_acc: 0.6828\n",
      "Epoch 11/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.6245 - acc: 0.6805 - val_loss: 0.6145 - val_acc: 0.6830\n",
      "Epoch 12/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.6242 - acc: 0.6796 - val_loss: 0.6195 - val_acc: 0.6828\n",
      "Epoch 13/50\n",
      "17954/17954 [==============================] - 139s 8ms/step - loss: 0.6253 - acc: 0.6799 - val_loss: 0.6190 - val_acc: 0.6830\n",
      "Epoch 14/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.6230 - acc: 0.6798 - val_loss: 0.6141 - val_acc: 0.6837\n",
      "Epoch 15/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.6214 - acc: 0.6794 - val_loss: 0.6191 - val_acc: 0.6823\n",
      "Epoch 16/50\n",
      "17954/17954 [==============================] - 141s 8ms/step - loss: 0.6205 - acc: 0.6803 - val_loss: 0.6169 - val_acc: 0.6834\n",
      "Epoch 17/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.6226 - acc: 0.6813 - val_loss: 0.6165 - val_acc: 0.6806\n",
      "Epoch 18/50\n",
      "17954/17954 [==============================] - 141s 8ms/step - loss: 0.6187 - acc: 0.6800 - val_loss: 0.6120 - val_acc: 0.6828\n",
      "Epoch 19/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.6219 - acc: 0.6817 - val_loss: 0.6160 - val_acc: 0.6837\n",
      "Epoch 20/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.6181 - acc: 0.6795 - val_loss: 0.6212 - val_acc: 0.6832\n",
      "Epoch 21/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.6158 - acc: 0.6819 - val_loss: 0.6208 - val_acc: 0.6823\n",
      "Epoch 22/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.6166 - acc: 0.6814 - val_loss: 0.6163 - val_acc: 0.6826\n",
      "Epoch 23/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.6147 - acc: 0.6842 - val_loss: 0.6137 - val_acc: 0.6823\n",
      "Epoch 24/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.6134 - acc: 0.6816 - val_loss: 0.6150 - val_acc: 0.6834\n",
      "Epoch 25/50\n",
      "17954/17954 [==============================] - 139s 8ms/step - loss: 0.6131 - acc: 0.6839 - val_loss: 0.6136 - val_acc: 0.6777\n",
      "Epoch 26/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.6139 - acc: 0.6846 - val_loss: 0.6174 - val_acc: 0.6823\n",
      "Epoch 27/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.6116 - acc: 0.6848 - val_loss: 0.6160 - val_acc: 0.6810\n",
      "Epoch 28/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.6095 - acc: 0.6853 - val_loss: 0.6244 - val_acc: 0.6794\n",
      "Epoch 29/50\n",
      "17954/17954 [==============================] - 139s 8ms/step - loss: 0.6104 - acc: 0.6847 - val_loss: 0.6202 - val_acc: 0.6772\n",
      "Epoch 30/50\n",
      "17954/17954 [==============================] - 141s 8ms/step - loss: 0.6081 - acc: 0.6893 - val_loss: 0.6195 - val_acc: 0.6788\n",
      "Epoch 31/50\n",
      "17954/17954 [==============================] - 141s 8ms/step - loss: 0.6066 - acc: 0.6877 - val_loss: 0.6131 - val_acc: 0.6855\n",
      "Epoch 32/50\n",
      "17954/17954 [==============================] - 139s 8ms/step - loss: 0.6045 - acc: 0.6886 - val_loss: 0.6216 - val_acc: 0.6770\n",
      "Epoch 33/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.6021 - acc: 0.6897 - val_loss: 0.6202 - val_acc: 0.6803\n",
      "Epoch 34/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.6034 - acc: 0.6879 - val_loss: 0.6202 - val_acc: 0.6810\n",
      "Epoch 35/50\n",
      "17954/17954 [==============================] - 139s 8ms/step - loss: 0.6016 - acc: 0.6909 - val_loss: 0.6237 - val_acc: 0.6792\n",
      "Epoch 36/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.5985 - acc: 0.6926 - val_loss: 0.6405 - val_acc: 0.6799\n",
      "Epoch 37/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.5987 - acc: 0.6937 - val_loss: 0.6269 - val_acc: 0.6712\n",
      "Epoch 38/50\n",
      "17954/17954 [==============================] - 139s 8ms/step - loss: 0.5948 - acc: 0.6960 - val_loss: 0.6300 - val_acc: 0.6734\n",
      "Epoch 39/50\n",
      "17954/17954 [==============================] - 139s 8ms/step - loss: 0.5948 - acc: 0.6954 - val_loss: 0.6289 - val_acc: 0.6783\n",
      "Epoch 40/50\n",
      "17954/17954 [==============================] - 139s 8ms/step - loss: 0.5937 - acc: 0.6964 - val_loss: 0.6341 - val_acc: 0.6679\n",
      "Epoch 41/50\n",
      "17954/17954 [==============================] - 139s 8ms/step - loss: 0.5891 - acc: 0.6969 - val_loss: 0.6287 - val_acc: 0.6814\n",
      "Epoch 42/50\n",
      "17954/17954 [==============================] - 139s 8ms/step - loss: 0.5873 - acc: 0.7012 - val_loss: 0.6378 - val_acc: 0.6641\n",
      "Epoch 43/50\n",
      "17954/17954 [==============================] - 139s 8ms/step - loss: 0.5857 - acc: 0.7001 - val_loss: 0.6440 - val_acc: 0.6788\n",
      "Epoch 44/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.5808 - acc: 0.7012 - val_loss: 0.6384 - val_acc: 0.6763\n",
      "Epoch 45/50\n",
      "17954/17954 [==============================] - 139s 8ms/step - loss: 0.5793 - acc: 0.7030 - val_loss: 0.6482 - val_acc: 0.6772\n",
      "Epoch 46/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.5799 - acc: 0.7047 - val_loss: 0.6490 - val_acc: 0.6712\n",
      "Epoch 47/50\n",
      "17954/17954 [==============================] - 139s 8ms/step - loss: 0.5769 - acc: 0.7074 - val_loss: 0.6544 - val_acc: 0.6734\n",
      "Epoch 48/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.5710 - acc: 0.7085 - val_loss: 0.6532 - val_acc: 0.6814\n",
      "Epoch 49/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.5682 - acc: 0.7118 - val_loss: 0.6778 - val_acc: 0.6745\n",
      "Epoch 50/50\n",
      "17954/17954 [==============================] - 140s 8ms/step - loss: 0.5680 - acc: 0.7123 - val_loss: 0.6750 - val_acc: 0.6683\n",
      "(22443, 40)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Reshape\n",
    "from  keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "\n",
    "\n",
    "## builds vgg9 model architechture\n",
    "model = build_model()\n",
    "\n",
    "## checks if pre-calculated weights are stored in a file\n",
    "exists = os.path.isfile('C:/Users/Komal/Desktop/classes/mlp/Kobe/my_model_weights.h5')\n",
    "if not exists:\n",
    "    compute_and_save_model_weights(model) ## If not stored then it computes new weights on the model that was built by training on train data\n",
    "\n",
    "model.load_weights('my_model_weights.h5') ## loads the weights into the model architecture\n",
    "\n",
    "model.pop() ## removes the fully connected layers\n",
    "#model.pop() \n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "bottleneck_features_train = model.predict(x_train)\n",
    "bottleneck_features_test = model.predict(x_test)\n",
    "print(np.shape(bottleneck_features_train))\n",
    "np.savetxt('bottleneck_features_train.csv', bottleneck_features_train, delimiter=\",\")\n",
    "np.savetxt('bottleneck_features_test.csv', bottleneck_features_test, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename= \"C:/Users/Komal/Desktop/classes/mlp/Kobe/bottleneck_features_train.csv\"\n",
    "train = pd.read_csv(filename, header=None)\n",
    "\n",
    "filename1= \"C:/Users/Komal/Desktop/classes/mlp/Kobe/bottleneck_features_test.csv\"\n",
    "test = pd.read_csv(filename1, header=None)\n",
    "\n",
    "#print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss(act, pred):\n",
    "\n",
    "    epsilon = 1e-15\n",
    "\n",
    "    pred = sp.maximum(epsilon, pred)\n",
    "\n",
    "    pred = sp.minimum(1-epsilon, pred)\n",
    "\n",
    "    ll = sum(act*sp.log(pred) + sp.subtract(1,act)*sp.log(sp.subtract(1,pred)))\n",
    "\n",
    "    ll = ll * -1.0/len(act)\n",
    "\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_RandomForest_parameters(train, plot=False):\n",
    "\n",
    "    # find the best n_estimators for RandomForestClassifier\n",
    "\n",
    "    print('Finding best n_estimators for RandomForestClassifier...')\n",
    "\n",
    "    min_score = 100000\n",
    "\n",
    "    best_n = 0\n",
    "\n",
    "    scores_n = []\n",
    "\n",
    "    range_n = np.logspace(0,2,num=3).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "    for n in range_n:\n",
    "\n",
    "        print(\"the number of trees : {0}\".format(n))\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "\n",
    "\n",
    "        rfc_score = 0.\n",
    "\n",
    "        rfc = RandomForestClassifier(n_estimators=n)\n",
    "\n",
    "        for train_k, test_k in KFold(len(train), n_folds=10, shuffle=True):\n",
    "\n",
    "            rfc.fit(train.iloc[train_k], y_train.iloc[train_k])\n",
    "\n",
    "            #rfc_score += rfc.score(train.iloc[test_k], train_y.iloc[test_k])/10\n",
    "\n",
    "            pred = rfc.predict(train.iloc[test_k])\n",
    "\n",
    "            rfc_score += logloss(y_train.iloc[test_k], pred) / 10\n",
    "\n",
    "        scores_n.append(rfc_score)\n",
    "\n",
    "        if rfc_score < min_score:\n",
    "\n",
    "            min_score = rfc_score\n",
    "\n",
    "            best_n = n\n",
    "\n",
    "\n",
    "\n",
    "        t2 = time.time()\n",
    "\n",
    "        print('Done processing {0} trees ({1:.3f}sec)'.format(n, t2-t1))\n",
    "\n",
    "    print(best_n, min_score)\n",
    "\n",
    "\n",
    "\n",
    "    # find best max_depth for RandomForestClassifier\n",
    "\n",
    "    print('Finding best max_depth for RandomForestClassifier...')\n",
    "\n",
    "    min_score = 100000\n",
    "\n",
    "    best_m = 0\n",
    "\n",
    "    scores_m = []\n",
    "\n",
    "    range_m = np.logspace(0,2,num=3).astype(int)\n",
    "\n",
    "    for m in range_m:\n",
    "\n",
    "        print(\"the max depth : {0}\".format(m))\n",
    "\n",
    "        t1 = time.time()\n",
    "\n",
    "\n",
    "\n",
    "        rfc_score = 0.\n",
    "\n",
    "        rfc = RandomForestClassifier(max_depth=m, n_estimators=best_n)\n",
    "\n",
    "        for train_k, test_k in KFold(len(train), n_folds=10, shuffle=True):\n",
    "\n",
    "            rfc.fit(train.iloc[train_k], y_train.iloc[train_k])\n",
    "\n",
    "            #rfc_score += rfc.score(train.iloc[test_k], train_y.iloc[test_k])/10\n",
    "\n",
    "            pred = rfc.predict(train.iloc[test_k])\n",
    "\n",
    "            rfc_score += logloss(y_train.iloc[test_k], pred) / 10\n",
    "\n",
    "        scores_m.append(rfc_score)\n",
    "\n",
    "        if rfc_score < min_score:\n",
    "\n",
    "            min_score = rfc_score\n",
    "\n",
    "            best_m = m\n",
    "\n",
    "\n",
    "\n",
    "        t2 = time.time()\n",
    "\n",
    "        print('Done processing {0} trees ({1:.3f}sec)'.format(m, t2-t1))\n",
    "\n",
    "    print(best_m, min_score)\n",
    "\n",
    "\n",
    "\n",
    "    if (plot):\n",
    "\n",
    "        plot_RandomForest_parameters(range_n, scores_n, range_m, scores_m)\n",
    "\n",
    "\n",
    "\n",
    "    return best_n, best_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_RandomForest_parameters(range_n, scores_n, range_m, scores_m):\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "\n",
    "    plt.subplot(121)\n",
    "\n",
    "    plt.plot(range_n, scores_n)\n",
    "\n",
    "    plt.ylabel('score')\n",
    "\n",
    "    plt.xlabel('number of trees')\n",
    "\n",
    "\n",
    "\n",
    "    plt.subplot(122)\n",
    "\n",
    "    plt.plot(range_m, scores_m)\n",
    "\n",
    "    plt.ylabel('score')\n",
    "\n",
    "    plt.xlabel('max depth')\n",
    "\n",
    "\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_RandomForest(train, train_y, test, best_n, best_m):\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=best_n, max_depth=best_m)\n",
    "\n",
    "    model.fit(train, train_y)\n",
    "\n",
    "    # pred_prob = model.predict_proba(train)\n",
    "\n",
    "    # pred_tes_probt = model.predict_proba(test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    pred_train = model.predict(train)\n",
    "\n",
    "    # pred = model.predict(submission)\n",
    "\n",
    "    pred_test = model.predict(test)\n",
    "\n",
    "\n",
    "\n",
    "    return pred_train, pred_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_result(pred_train, train_y, pred_test, test_y):\n",
    "\n",
    "    # Calculate the absolute errors\n",
    "\n",
    "    errors = abs(pred_train - train_y)\n",
    "\n",
    "    # Print out the mean absolute error (mae)\n",
    "\n",
    "    print('Mean Absolute Train Error:', round(np.mean(errors), 2), 'degrees.')\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate mean absolute percentage error (MAPE)\n",
    "\n",
    "    mape = 100 * (errors)\n",
    "\n",
    "    # Calculate and display accuracy\n",
    "\n",
    "    accuracy = 100 - np.mean(mape)\n",
    "\n",
    "    print('Train Accuracy:', round(accuracy, 2), '%.')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate the absolute errors\n",
    "\n",
    "    errors1 = abs(pred_test - test_y)\n",
    "\n",
    "    # Print out the mean absolute error (mae)\n",
    "\n",
    "    print('Mean Absolute Test Error:', round(np.mean(errors1), 2), 'degrees.')\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate mean absolute percentage error (MAPE)\n",
    "\n",
    "    mape = 100 * (errors1)\n",
    "\n",
    "    # Calculate and display accuracy\n",
    "\n",
    "    accuracy1 = 100 - np.mean(mape)\n",
    "\n",
    "    print('Test Accuracy:', round(accuracy1, 2), '%.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Komal\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best n_estimators for RandomForestClassifier...\n",
      "the number of trees : 1\n",
      "Done processing 1 trees (1.031sec)\n",
      "the number of trees : 10\n",
      "Done processing 10 trees (9.083sec)\n",
      "the number of trees : 100\n",
      "Done processing 100 trees (82.954sec)\n",
      "100 10.494211345901112\n",
      "Finding best max_depth for RandomForestClassifier...\n",
      "the max depth : 1\n",
      "Done processing 1 trees (8.325sec)\n",
      "the max depth : 10\n",
      "Done processing 10 trees (46.432sec)\n",
      "the max depth : 100\n",
      "Done processing 100 trees (88.331sec)\n",
      "10 9.775495554560752\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import KFold   #from sklearn.model_selection import KFold\n",
    "\n",
    "best_n, best_m = find_RandomForest_parameters(train)\n",
    "\n",
    "pred_train, pred_test = run_RandomForest(train, y_train, test, best_n, best_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Train Error: 0.23 degrees.\n",
      "Train Accuracy: 76.54 %.\n",
      "Mean Absolute Test Error: 0.36 degrees.\n",
      "Test Accuracy: 64.18 %.\n"
     ]
    }
   ],
   "source": [
    "predictions_result(pred_train, y_train, pred_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
